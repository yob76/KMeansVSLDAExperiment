# -*- coding: utf-8 -*-
"""Assignment5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s7KyJ_uKnev7W1XDffjOiisAVKwldcEV

Name: Yasmeen Bashadi EID: yob76
"""

import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
!pip install gensim
import gensim.downloader as api
import nltk
import pandas as pd
import re
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')

glove_model = api.load("glove-twitter-50")
file_path = "News_tweets.txt"

#text preprocessing function
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", "", text)  # remove URLs
    text = re.sub(r"[^a-z\s]", "", text)  # remove special characters
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stopwords.words('english') and len(word) > 2]
    return tokens

#preprocess tweets and extract unique tokens
processed_tweets = [preprocess_text(tweet) for tweet in tweets]
unique_tokens = list(set([word for tweet in processed_tweets for word in tweet]))

word_vectors = np.array([glove_model[word] for word in unique_tokens if word in glove_model])

#KMeans clustering and keyword analysis
def kmeans_clustering(k):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(word_vectors)

    # organize words by cluster
    clusters = {i: [] for i in range(k)}
    for word, label in zip(unique_tokens, labels):
        clusters[label].append(word)

    return clusters

# clustering for K=2, K=5, and K=8
clusters_k2 = kmeans_clustering(2)
clusters_k5 = kmeans_clustering(5)
clusters_k8 = kmeans_clustering(8)

vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)
tweet_term_matrix = vectorizer.fit_transform(processed_tweets)

#run LDA and extract topics
def lda_topic_modeling(num_topics):
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(tweet_term_matrix)

    keywords_per_topic = {}
    for topic_idx, topic in enumerate(lda.components_):
        top_keywords = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]
        keywords_per_topic[topic_idx] = top_keywords

    return keywords_per_topic

# LDA for num_topics=2, 5, 8
lda_topics_2 = lda_topic_modeling(2)
lda_topics_5 = lda_topic_modeling(5)
lda_topics_8 = lda_topic_modeling(8)

#Results
print("KMeans Clustering Results:")
print("K=2:", clusters_k2)
print("K=5:", clusters_k5)
print("K=8:", clusters_k8)

print("\nLDA Topic Modeling Results:")
print("Topics=2:", lda_topics_2)
print("Topics=5:", lda_topics_5)
print("Topics=8:", lda_topics_8)

"""Observations: The results from KMeans clustering and LDA topic modeling provide a clear view of how keywords in the dataset are grouped and what topics they might represent.

Starting with KMeans clustering, when we set K=2, the model broadly separates words into two groups. Cluster 0 includes words like rule, duncan, ncaa, tool, mayans, and fish, suggesting a mix of general terms and specific entities. As we increase to K=5, the clusters start to take on more specific themes, with words like duncan, mayans, fish, papers, wring, and emergency appearing together, which might indicate discussions around particular events or crises. At K=8, the words become even more specialized, with terms like bounds, prizewinning, grading, and sarah possibly pointing to topics related to achievements, evaluations, or individuals. This progression shows that as we increase the number of clusters, we get more refined groupings, helping to uncover more detailed themes in the dataset.

LDA topic modeling, on the other hand, looks at word co-occurrence to identify topics in the text. With two topics, we see a strong focus on COVID-19 statistics and the virus itself. One topic includes words like reported, deaths, number, total, states, and tests, which suggests discussions around case numbers and pandemic updates. The second topic has words like virus, spread, new, health, and coronavirus, indicating conversations about the disease and its transmission. When we expand to five topics, the themes become more distinct, with words like people, hospital, recoveries, and testing pointing to healthcare-related discussions, while indiafightscorona, trump, and India hint at political or governmental messaging. Finally, at eight topics, the results get even more specific, with clusters related to prevention such as use, prevent, and spread and statistics like cases, lakh, recoveries, tests, and new, further breaking down the datasetâ€™s key discussions.

Comparing the two methods, KMeans clustering groups words based on similarity, which helps in organizing related terms, while LDA topic modeling extracts broader themes based on word distributions. KMeans gives a more structured way to see relationships between words, whereas LDA helps us understand the underlying discussions within the dataset. Together, they provide a solid way to analyze and interpret keyword patterns in the text.
"""